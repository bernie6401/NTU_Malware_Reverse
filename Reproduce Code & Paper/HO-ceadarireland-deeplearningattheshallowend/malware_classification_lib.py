######################################################################################################
# The code is the work of CeADAR
# If you use the code please cite our paper:
# Le, Quan; Boydell, Oisin; Mac Namee, Brian; Scanlon, Mark - 'Deep Learning at the Shallow End: Malware 
# Classification for Non-Domain Experts'. Digital Investigation (2018)
# https://doi.org/10.1016/j.diin.2018.04.024
######################################################################################################


import os
import numpy as np

from keras import models 
from keras import layers
from keras import regularizers

from copy import deepcopy

def c_2_hex():
    """
    generate a dictionary that maps from
    a character to its corresponding hexa digit (0 to 15)
    """
    c2hex_ = {}
    for i in range(10):
        c2hex_[str(i)] = i
    # assign value for 'A' to 'F'
    for i in range(6):
        c2hex_[chr(ord('A') + i)] = 10 + i

    return c2hex_



STOP_CHAR = '?'
HEX_BASE = 16
c2hex = c_2_hex()


def list_dir(directory, suffix='bin'):
    """
    return the list of file names with ending suffix
    Params:
        directory: directory to search files
        suffix: the suffix of the file names to have in the results
    return:
        The list of chosen file names, only filenames but not the file paths to them.
    """

    files = os.listdir(directory)

    chosen_fns = []
    for fn in files:
        if fn.endswith(suffix):
            chosen_fns.append(fn)

    return chosen_fns




def convert_bytes_f_2_bin(byte_fn, bin_fn, byte_buffer):
    """
    convert a file in .bytes format to .bin format
    Params:
        byte_fn: file in .bytes format with path to it
        bin_fn: file in the binary format, with path to it
                the converted byte stream will be stored here
        byte_buffer: a nd_array variable of shape (MAX_SIZE,) in uint8 type to store 
                    byte stream when converting. The length of 
                    byte_buffer will limit how many converted bytes will have
                    in the byte_fn file
    
    """
    # NOTE: format of the .bytes file
    # each line in a .bytes file consists first a hex number indicating the file position
    # of the bytes of the file
    # the rest of the line are words indicating bytes in the binary program
    # each word corresponds to a number between 0 to 255 in hex format
    # each word = 2 letter of the hex digits (0-9, A-F)
    # near the end of the file there are words of two '??'
    # corresponding to contents that will be determined when the program
    # is loaded to memory (e.g. variables values..)
    # NOTE: For now I will ignore all ?? words and only convert the non '??' ones
    # 


    byte_idx = 0
    max_len = byte_buffer.shape[0]

    with open(byte_fn, 'rt') as byte_f:
        lines = byte_f.readlines()

    stop_parsing = False

    for line in lines:
        # split the line, remove the first element which is its position in the binary file
        # then convert every two letters in the the rest into a byte value, assign that value
        # to the corresponding position in byte_buffer
        hex_words = line.strip().split()
        for hex_word in hex_words[1:]:
            if len(hex_word) > 2 :
                raise Exception('In file {:s}, length of hexword {:s} is bigger than 2'.format(
                    byte_fn, hex_word))
            # convert hexadecimal words to byte value
            if hex_word[0] not in c2hex or hex_word[1] not in c2hex:
                # Ignore the hex word and go to the next one
                # NOTE: might have to change this in the future
                continue
            hex_digit1, hex_digit2 = c2hex[hex_word[0]], c2hex[hex_word[1]]
            byte_val = hex_digit1 * HEX_BASE + hex_digit2
            byte_buffer[byte_idx] = byte_val
            byte_idx += 1

            if byte_idx == max_len:
                stop_parsing = True
                break
        if stop_parsing:
            break

    # byte_idx is the length of the byte stream to be created
    byte_stream = np.zeros(shape=(byte_idx, ), dtype='uint8')
    np.copyto(byte_stream, byte_buffer[:byte_idx])
    byte_stream.tofile(bin_fn)
        



def create_cnn_lstm_model(max_len, n_class, bidirectional=True):
    """
    generate CNN with 1 or Bi directional LSTM on top 
    Parameters:
        max_len: length of the input sequence
        n_class: number of classes
        bidirectional: boolean value: to generate uni or bidirectional LSTM on top of CNN layers
    Return:
        the CNN -UniLSTM or CNN-BiLSTM model
    """
    cnn_lstm_model = models.Sequential()

    cnn_lstm_model.add(layers.Conv1D(filters=30, kernel_size=7, strides=1, kernel_regularizer=regularizers.l2(0.01), activation='relu', input_shape=(max_len, 1)))
    cnn_lstm_model.add(layers.MaxPool1D(5))
    cnn_lstm_model.add(layers.Conv1D(filters=50, kernel_size=7, strides=1, kernel_regularizer=regularizers.l2(0.01), activation='relu'))
    cnn_lstm_model.add(layers.MaxPool1D(5))
    cnn_lstm_model.add(layers.Conv1D(filters=90, kernel_size=7, strides=1, kernel_regularizer=regularizers.l2(0.01), activation='relu'))
    cnn_lstm_model.add(layers.MaxPool1D(5))
    
    if bidirectional:
        cnn_lstm_model.add(layers.Bidirectional(layers.LSTM(units=128, dropout=0.2, recurrent_dropout=0.2)))
    else:
        cnn_lstm_model.add(layers.LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
    
    cnn_lstm_model.add(layers.Dense(n_class, activation='softmax'))

    cnn_lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return cnn_lstm_model


def create_cnn_model(max_len, n_class):
    """
    generate the CNN model
    Parameters: 
        max_len: lengthe of the input sequence
        n_class: number of classes
    Return:
        the CNN model
    """
    cnn_model_ = models.Sequential()

    cnn_model_.add(layers.Conv1D(filters=30, kernel_size=7, strides=1, kernel_regularizer=regularizers.l2(0.01), activation='relu', input_shape=(max_len, 1)))
    cnn_model_.add(layers.MaxPool1D(5))
    cnn_model_.add(layers.Conv1D(filters=50, kernel_size=7, strides=1, kernel_regularizer=regularizers.l2(0.01), activation='relu'))
    cnn_model_.add(layers.MaxPool1D(5))
    cnn_model_.add(layers.Conv1D(filters=90, kernel_size=7, strides=1, kernel_regularizer=regularizers.l2(0.01), activation='relu'))
    cnn_model_.add(layers.MaxPool1D(5))

    cnn_model_.add(layers.Flatten())
    cnn_model_.add(layers.Dropout(0.2))
    cnn_model_.add(layers.Dense(256, activation='relu'))
    cnn_model_.add(layers.Dropout(0.3))
    cnn_model_.add(layers.Dense(n_class, activation='softmax'))

    cnn_model_.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return cnn_model_




# NOTE: y_train here is a list of size n_samples, y_train_cat corresponds to y_train in the other script

def calc_cls_2_idxs(targets, n_class):
    """
    calculate the dictionary which maps from a class label to the array of 
    indexes with corresponding labels
    targets has dimension (n_examples,)
    i.e. not in the one hot encoding
    """
    cls_2_idxs= {}
    for i in range(n_class):
        cls_2_idxs[i] = []
    for sample_idx, sample_cls in enumerate(targets):
        cls_2_idxs[sample_cls].append(sample_idx)

    for i in range(n_class):
        cls_2_idxs[i] = np.array(cls_2_idxs[i])

    return cls_2_idxs




# The cls_2_idxs_train should be shuffleed before putting into cv_train_valid_generator

def cv_train_valid_generator(data, targets, cls_2_idxs, kfold):
    """
    Given data, targets tensors, cls_2_idxs that maps from one class to all the indexes of samples
    belonging to it. Generate kfold batches of train, valid split of data and targets
    in the cross-validation protocol so that the distribution of each class in train and set is the same
    as in data, targets
    Parameters:
        data: data tensors (nexamples, n_features, 1)
        targets: targets (nexamples, n_class)
        cls_2_idxs: the dictionary that maps from each class to the set of example indexes whose class is it (np.array)
    Return:
        ((train_data_split, train_target_split, orig_ex_idxs_train ), (valid_data_split, valid_target_split, orig_ex_idxs_valid))
        orig_ex_idxs_train[0] is the index of the sample 0 in the train split in data,..
        orig_ex_idxs_valid[0] is the index of the sample 0 in the valid split in data,..
    """

    # assume each class has more than kfold examples
    cls_2_nexs = {cls_label: len(cls_idx_l) for cls_label, cls_idx_l in cls_2_idxs.items()}
    cls_2_nexs_fold = {}
    for cls_label in cls_2_idxs.keys():
        cls_2_nexs_fold[cls_label] = len(cls_2_idxs[cls_label])// kfold

    for kfold_idx in range(kfold):
        # generate train, valid split, yield the two parts
        # get the start_idx, end_idx for the validation split for each class (pos of range os exs in the list of ex idxs
        # belonging to the class
        cls_2_v_start_end = {}
        for cls_label in cls_2_idxs.keys():
            fold_nexs_cls = cls_2_nexs_fold[cls_label]
            # min function is used to make sure the range of validation indexes are not out of bound in
            # cls_2_idxs[cls_label] list
            if kfold_idx < kfold-1:
                cls_2_v_start_end[cls_label] = [fold_nexs_cls * kfold_idx, min(fold_nexs_cls * (kfold_idx+1),
                                                                               cls_2_nexs[cls_label])]
            elif kfold_idx == kfold -1:
                # last split might have more than number of example in a class // kfold (add the modulo after the
                # integer division)
                cls_2_v_start_end[cls_label] = [fold_nexs_cls * kfold_idx, cls_2_nexs[cls_label]]

        cls_2_idxs_v = {cls_label: cls_2_idxs[cls_label][start_pos: end_pos] for cls_label, [start_pos, end_pos] in
                        cls_2_v_start_end.items()}


        cls_2_idxs_train = {cls_label: np.append(cls_2_idxs[cls_label][:start_pos], cls_2_idxs[cls_label][end_pos:],
                                                 axis=0) for cls_label, [start_pos, end_pos] in
                        cls_2_v_start_end.items()}
        
        n_train_split = sum([len(cls_2_idxs_train[cls_label]) for cls_label in cls_2_idxs_train])
        n_valid_split = sum([len(cls_2_idxs_v[cls_label]) for cls_label in cls_2_idxs_v])

        data_train = np.zeros((n_train_split,) + tuple(data.shape[1:]))
        target_train = np.zeros((n_train_split, targets.shape[-1]))

        
        data_valid = np.zeros((n_valid_split,) + tuple(data.shape[1:]))
        target_valid = np.zeros((n_valid_split, targets.shape[-1]))
                        
        # fill the data, and targets for both train and valid splits            
        all_cls_labels = list(cls_2_idxs.keys())

        # end_pos for each cls_label in the train or valid split
        
        # array corresponding to all_cls_labels array, storing the number of sample in each class
        cls_label_ntrains = [len(cls_2_idxs_train[cls_label]) for cls_label in all_cls_labels]
        cls_label_nvalids = [len(cls_2_idxs_v[cls_label]) for cls_label in all_cls_labels]

        # samples for each class will be in a consecutive segment of indexs in the train, valid split, 
        # will shuffle them later
        cls_label_endpos_train = np.cumsum(cls_label_ntrains)
        cls_label_endpos_valid = np.cumsum(cls_label_nvalids)

        orig_ex_idxs_train = np.zeros((n_train_split,), dtype=int)
        orig_ex_idxs_valid = np.zeros((n_valid_split,), dtype=int)

        # populate the data and targets for train and test splits
        for idx, cls_label in enumerate(all_cls_labels):
            if idx == 0:
                start_pos_train = 0
                start_pos_valid = 0
            else: 
                start_pos_train = cls_label_endpos_train[idx-1]
                start_pos_valid = cls_label_endpos_valid[idx-1]

            end_pos_train = cls_label_endpos_train[idx]
            end_pos_valid = cls_label_endpos_valid[idx]
            
            data_train[start_pos_train:end_pos_train] = data[cls_2_idxs_train[cls_label]]
            target_train[start_pos_train:end_pos_train] = targets[cls_2_idxs_train[cls_label]]
            orig_ex_idxs_train[start_pos_train:end_pos_train] = cls_2_idxs_train[cls_label]


            data_valid[start_pos_valid:end_pos_valid] = data[cls_2_idxs_v[cls_label]]
            target_valid[start_pos_valid:end_pos_valid] = targets[cls_2_idxs_v[cls_label]]
            orig_ex_idxs_valid[start_pos_valid:end_pos_valid] = cls_2_idxs_v[cls_label]

        # shuffle the train and test splits
        shuffled_idxs_tr = list(range(n_train_split))
        np.random.shuffle(shuffled_idxs_tr)
        shuffled_idxs_v = list(range(n_valid_split))
        np.random.shuffle(shuffled_idxs_v)

        data_train = data_train[shuffled_idxs_tr]
        target_train = target_train[shuffled_idxs_tr]
        orig_ex_idxs_train = orig_ex_idxs_train[shuffled_idxs_tr]

        data_valid = data_valid[shuffled_idxs_v]
        target_valid = target_valid[shuffled_idxs_v]
        orig_ex_idxs_valid = orig_ex_idxs_valid[shuffled_idxs_v]

        # need to recover the real indexes of the train and valid split from data

        yield (data_train, target_train, orig_ex_idxs_train), (data_valid, target_valid, orig_ex_idxs_valid)







def data_batch_generator(data,  targets, cls_2_ex_idxs, shuffle=False, batch_size=128 ):
    """
    To generate a batch_size examples (if possible, might be less) of examples and targets
    in each step
    This design is for imbalanced dataset. Even different classes in data has different 
    distributions, generate approximately batch_size//n_class examples for each class

    Assumptions: each class has at least batch_size//n_class + 1 examples

    Parameters:
        data: 3D tensor storing data ex_idx dimension, image length for 1 malware (10k), 1
        targets: tensor specifying target for each example (malware file), (n_examples, n_class), or (n_examples)
        cls_2_ex_idxs: dictionary mapping from each class label to the array (np.array) of example indexes (whose class is it)
        shuffle: shuffle the examples or not
        batch_size

    Return: a batch of  batch_size examples (if shuffle=True batch_size) in the form samples, samples_target

    """
    cls_2_ex_idxs_cp = deepcopy(cls_2_ex_idxs)
    all_class_labels = list(cls_2_ex_idxs_cp.keys())
    n_class = len(all_class_labels)
    cls_2_nexs = {cls_label: len(ex_idxs_4_cls) for cls_label, ex_idxs_4_cls in cls_2_ex_idxs_cp.items()}
    cls_2_currentpos = {cls_label:0 for cls_label in all_class_labels}


    bsize_1cls_appx = batch_size // n_class
    remainder = batch_size % n_class
    # array storing the number of examples of each class in the batch
    bsize_arr = [bsize_1cls_appx] * (n_class -remainder) + [bsize_1cls_appx + 1] * remainder 

    # map from one class label to the number of examples in that class in the batch
    bsize_of_cls = {all_class_labels[i]: bsize_arr[i] for i in range(len(bsize_arr))}
    
    while 1:
        for cls_label in all_class_labels:
            if cls_2_currentpos[cls_label] + bsize_of_cls[cls_label] > cls_2_nexs[cls_label]:
                cls_2_currentpos[cls_label] = 0
                #NOTE: changing the code to use shuffle here is different from the previous version 
                # of data_batch_generator, check the previous version in git to see the diff
                if shuffle:
                    np.random.shuffle(cls_2_ex_idxs_cp[cls_label])

        # indexes of examples chosen for each class in the batch
        cls_2_rows = {cls_label: 
                      np.arange(cls_2_currentpos[cls_label], 
                                min(cls_2_currentpos[cls_label] + bsize_of_cls[cls_label], cls_2_nexs[cls_label]))
                      for cls_label in all_class_labels}

        cls_2_currentpos = {cls_label: cls_2_currentpos[cls_label] + len(cls_2_rows[cls_label])
                            for cls_label in all_class_labels}


        n_total_b_size = np.sum([len(cls_2_rows[cls_label]) for cls_label in all_class_labels])
        
        # should use shuffled_idxs on both samples and sample_targets
        samples = np.zeros((n_total_b_size,) + tuple(data.shape[1:]))

        sample_targets = np.zeros((n_total_b_size,) + tuple(targets.shape[1:]))

        # use the bsize_cls_arr[i] corresponds to all_class_labels[i] class
        bsize_cls_arr = [len(cls_2_rows[cls_label]) for cls_label in all_class_labels]
        # examples of each class will first be in a consecutive segment of the batch
        # class label order follow all_class_labels
        cls_endpos_in_b = np.cumsum(bsize_cls_arr)
       
        # populate samples in the batch
        for idx, cls_label in enumerate(all_class_labels):
            if idx == 0:
                start_pos = 0
            else:
                start_pos = cls_endpos_in_b[idx-1]

            end_pos = cls_endpos_in_b[idx]
            b_ex_idxs_4_cls = cls_2_ex_idxs_cp[cls_label][cls_2_rows[cls_label]]
            samples[start_pos:end_pos] = data[b_ex_idxs_4_cls]
            sample_targets[start_pos:end_pos] = targets[b_ex_idxs_4_cls]


        # then shuffle the data
        shuffled_idxs = list(range(n_total_b_size))
        np.random.shuffle(shuffled_idxs)

        samples = samples[shuffled_idxs]
        sample_targets = sample_targets[shuffled_idxs]
        yield samples, sample_targets


