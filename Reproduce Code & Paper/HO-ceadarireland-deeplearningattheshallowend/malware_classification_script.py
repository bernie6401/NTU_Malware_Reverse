######################################################################################################
# The code is the work of CeADAR
# If you use the code please cite our paper:
# Le, Quan; Boydell, Oisin; Mac Namee, Brian; Scanlon, Mark - 'Deep Learning at the Shallow End: Malware 
# Classification for Non-Domain Experts'. Digital Investigation (2018)
# https://doi.org/10.1016/j.diin.2018.04.024
######################################################################################################


# Download data from https://www.kaggle.com/c/malware-classification/data and save in the directory data,
# Unzip files train.7z and test.z 

import tensorflow as tf
import keras          
from keras.utils import to_categorical


# Uncomment this block to train the model on GPU

#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3) 
#config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=True, device_count={'GPU': 1})
#session = tf.Session(config=config) 
#keras.backend.set_session(session) 


import numpy as np
import cv2
import os
import pandas as pd
import csv
import sklearn.metrics as metrics

from copy import deepcopy

#import pickle


import malware_classification_lib as mc_lib


import time

project_dir = '.'

data_dir = os.path.join(project_dir, 'data')
#data_dir = os.path.join(project_dir, 'data_test')

train_dir = os.path.join(data_dir, 'train')
test_dir = os.path.join(data_dir, 'test')

model_dir = os.path.join(project_dir, 'model')

train_labels_fn = os.path.join(data_dir, 'trainLabels.csv')
train_labels_df = pd.read_csv(train_labels_fn)
# count the number of examples for each label
train_labels_df.groupby('Class').count()

batch_size = 64


# For kfold crossvalidation procedure
kfold = 5



###################################################
# For train, test data - Convert each binary file .bytes to binary format .bin
# in the subdirectory ./bin
# the .bytes files are the text format of the binary file
###################################################


# size of the biggest binary file is 70 Mbytes
# found on the train and test set to be 54MBytes before converting to bin format
# du -a -b ./*.bytes | sort -n
MAX_SIZE = 6* int(1e7)
byte_buffer = np.zeros((MAX_SIZE,), dtype='uint8') # to buffer the byte stream of 1 file when converting the text stream to it





byte_sfx = 'bytes' # for .bytes file text file with bin content
bin_sfx = 'bin'

bin_subdir = 'bin'

# Convert bytes. files to bin file for training data
train_bin_dir = os.path.join(train_dir, bin_subdir)
if not os.path.exists(train_bin_dir):
    os.makedirs(train_bin_dir)

train_byte_fns = mc_lib.list_dir(train_dir, suffix=byte_sfx)

train_byte_fn2id = {fn:fn[:-len(byte_sfx) -1] for fn in train_byte_fns}

# Set to True once the conversion is done
binf_train_created = False
if not binf_train_created:
    for idx, byte_fn in enumerate(train_byte_fns):
        byte_fn_wp = os.path.join(train_dir, byte_fn)
        bin_fn = train_byte_fn2id[byte_fn] + '.' + bin_sfx
        bin_fn_wp = os.path.join(train_bin_dir, bin_fn)
        mc_lib.convert_bytes_f_2_bin(byte_fn_wp, bin_fn_wp, byte_buffer=byte_buffer)
        if idx % 500 == 0:
            print('Processing until file number {:d}'.format(idx))

    binf_train_created = True


test2submit_bin_dir = os.path.join(test_dir, bin_subdir)
if not os.path.exists(test2submit_bin_dir):
    os.makedirs(test2submit_bin_dir)

test2submit_byte_fns = mc_lib.list_dir(test_dir, suffix=byte_sfx)

test2submit_byte_fn2id = {fn:fn[:-len(byte_sfx) -1] for fn in test2submit_byte_fns}

# Set to True once the conversion is done
binf_test2submit_created = False
if not binf_test2submit_created:
    for idx, byte_fn in enumerate(test2submit_byte_fns):
        byte_fn_wp = os.path.join(test_dir, byte_fn)
        bin_fn = test2submit_byte_fn2id[byte_fn] + '.' + bin_sfx
        bin_fn_wp = os.path.join(test2submit_bin_dir, bin_fn)
        mc_lib.convert_bytes_f_2_bin(byte_fn_wp, bin_fn_wp, byte_buffer=byte_buffer)
        if idx % 500 == 0:
            print('Processing until file number {:d}'.format(idx))

    binf_test2submit_created = True


###################################################
# For train data and test data to submit to kaggle server - Convert each binary file to a smaller binary file - 
# each as a sequence of bytes of a specified length
# 
###################################################

bin_sfx = 'bin'
max_len = int(1e4)
width = 1 # image as a stream of bytes

img_sfx = ''.join(['img', 'w', str(width), 'h', str(max_len)]) # image with width 1 -> the image is a stream of bytes

img_subdir = ''.join(['img', '_w', str(width), '_h', str(max_len)])


train_img_output_dir = os.path.join(train_dir, img_subdir  )
if not os.path.exists(train_img_output_dir):
    os.makedirs(train_img_output_dir)



train_bin_fns = mc_lib.list_dir(train_bin_dir, suffix=bin_sfx)



train_binfn2id = {fn:fn[:-(len(bin_sfx) + 1)] for fn in train_bin_fns}

binfn_id2cls = {} # file name id is the part before .
for fn_label_item in train_labels_df.itertuples():
    binfn_id2cls[fn_label_item.Id ] = fn_label_item.Class



start_train_img_prepare = time.time()

# set to True once the image files from the train set is created
train_img_created = False

if not train_img_created:
    for idx, fn in enumerate(train_bin_fns):
        fn_wp = os.path.join(train_bin_dir, fn)
        bin_stream = np.fromfile(fn_wp, dtype='uint8')
        if bin_stream.shape[0] == 0:
            #size 0 file
            print('file with size 0')
            print(fn)
            continue
        if idx == 0:
            print(bin_stream.shape)
        if idx % 500 == 0:
            print('processing until file idx', idx)

        # reshape the bin stream to (bin_str_len, 1)
        bin_stream = bin_stream.reshape(bin_stream.shape[0], 1)
        if idx == 0:
            print('After reshaping', bin_stream.shape)


        # NOTE: if I use width different from 1 I need to reshape bin_stream appropriately
        img_shrink = cv2.resize(bin_stream, (width, max_len))

        img_fn = train_binfn2id[fn] + '.' + img_sfx
        img_fn_wp = os.path.join(train_img_output_dir, img_fn)
        img_shrink.tofile(img_fn_wp)


    train_img_created = True

end_train_img_prepare = time.time()

# Create image for test2 submit bin files

test2submit_img_o_dir = os.path.join(test_dir, img_subdir )
if not os.path.exists(test2submit_img_o_dir):
    os.makedirs(test2submit_img_o_dir)



test2submit_bin_fns = mc_lib.list_dir(test2submit_bin_dir, suffix=bin_sfx)



test2submit_binfn2id = {fn:fn[:-(len(bin_sfx) + 1)] for fn in test2submit_bin_fns}


# The test 2 submit data has no label, 

start_test_img_prepare = time.time()

#NOTE some img bin file has no code, size 0. The list is in an annotation file in ./data
# no image file is produced for these files

# set to True once the image files from the train set is created
test2submit_img_created = False

if not test2submit_img_created:
    for idx, fn in enumerate(test2submit_bin_fns):
        fn_wp = os.path.join(test2submit_bin_dir, fn)
        bin_stream = np.fromfile(fn_wp, dtype='uint8')
        if bin_stream.shape[0] == 0:
            #size 0 file
            print('file with size 0')
            print(fn)
            continue
        if idx == 0:
            print(bin_stream.shape)
        if idx % 500 == 0:
            print('processing until file idx', idx)

        # reshape the bin stream to (bin_str_len, 1)
        bin_stream = bin_stream.reshape(bin_stream.shape[0], 1)
        if idx == 0:
            print('After reshaping', bin_stream.shape)


        img_shrink = cv2.resize(bin_stream, (width, max_len))

        img_fn = test2submit_binfn2id[fn] + '.' + img_sfx
        img_fn_wp = os.path.join(test2submit_img_o_dir, img_fn)
        img_shrink.tofile(img_fn_wp)

    test2submit_img_created = True

end_test_image_prepare = time.time()

#############################################################
# Load image data  from the training set
#############################################################
# NOTE: default value width = 1

train_img_dir = train_img_output_dir

img_fns = mc_lib.list_dir(train_img_dir, suffix=img_sfx)

n_images = len(img_fns)

x_train = np.zeros(shape=(n_images, max_len, 1), dtype=np.float32)

imgfn2id= {fn:fn[:-(len(img_sfx) + 1)] for fn in img_fns}

imgfn2class = {img_fn: binfn_id2cls.get(imgfn2id.get(img_fn)) for img_fn in img_fns}

y_train = [-1] * n_images



for idx, img_fn in enumerate(img_fns):
    img_fn_wp = os.path.join(train_img_dir, img_fn)
    img_bin_stream = np.fromfile(img_fn_wp, dtype='uint8')
    x_train[idx, :, :] = img_bin_stream.reshape(max_len, 1) # Assume they have shape (max_len, 1)
    # NOTE: i deduct 1 from the class (1 to 9) to get the values from 0 to 8
    y_train[idx] = imgfn2class[img_fn] - 1


N_CLASS = 9 # 0 to 8 after substraction
y_train_cat = to_categorical(y_train)







cls_2_idxs_train = mc_lib.calc_cls_2_idxs(y_train, n_class=N_CLASS)

for cls_label in cls_2_idxs_train:
    np.random.shuffle(cls_2_idxs_train[cls_label])


####################################################################################################
# 5 fold cross-validation procedure on the CNN-BiLSTM model using the class rebalancing data batch generator
#################################################################################################### 




exs_pred_cnnlstm_gen = np.zeros((x_train.shape[0], N_CLASS))
for tr_split_tuple, v_split_tuple in mc_lib.cv_train_valid_generator(x_train, y_train_cat, cls_2_idxs_train, kfold=kfold):
    print(v_split_tuple[1].shape)
    x_train_fold, y_train_cat_fold, orig_ex_idxs_train_f = tr_split_tuple
    x_valid_fold, y_valid_cat_fold, orig_ex_idxs_valid_f  = v_split_tuple


    cls_2_idxs_train_f = mc_lib.calc_cls_2_idxs(np.argmax(y_train_cat_fold, axis=1), n_class=N_CLASS)

    train_gen_f = mc_lib.data_batch_generator(x_train_fold, y_train_cat_fold, cls_2_idxs_train_f, shuffle=True,
                                     batch_size=batch_size)

    train_steps_f = 2 * x_train_fold.shape[0] // batch_size

    
    cnnlstm_model_fold = mc_lib.create_cnn_lstm_model(max_len=max_len, n_class=N_CLASS)
    history_cnnlstm_m_f = cnnlstm_model_fold.fit_generator(train_gen_f, steps_per_epoch=train_steps_f, 
                    epochs = 100, validation_data=(x_train_fold, y_train_cat_fold))  # default value is 100


    # record the prediction on the validation fold
    valid_exs_pred = cnnlstm_model_fold.predict(x_valid_fold)
    exs_pred_cnnlstm_gen[orig_ex_idxs_valid_f] = valid_exs_pred




exs_pred_cnnlstm_gen_cls = np.argmax(exs_pred_cnnlstm_gen, axis=1)

# Report the accuracy and macro average f1 score
metrics.accuracy_score(y_train, exs_pred_cnnlstm_gen_cls)
metrics.f1_score(y_train, exs_pred_cnnlstm_gen_cls, average='macro')




########################################
# Train final model to submit results to kaggle
########################################


cls_2_idxs_train_cp = deepcopy(cls_2_idxs_train)

for cls_label in cls_2_idxs_train_cp:
    np.random.shuffle(cls_2_idxs_train_cp[cls_label])
    np.random.shuffle(cls_2_idxs_train_cp[cls_label])

# Generate a train valid model where 90% of the data is for training, and 10% for testing

final_train_nsplits = 10
final_train_valid_gen = mc_lib.cv_train_valid_generator(x_train, y_train_cat, cls_2_idxs_train_cp, kfold=final_train_nsplits)

final_train_split_tuple, final_valid_split_tuple = next(final_train_valid_gen)

final_x_train_s, final_y_train_cat_s, orig_idxs_final_train_s = final_train_split_tuple
final_x_valid_s, final_y_valid_cat_s, orig_idxs_final_valid_s = final_valid_split_tuple


cls_2_idx_final_train = mc_lib.calc_cls_2_idxs(np.argmax(final_y_train_cat_s, axis=1), n_class=N_CLASS)

train_gen = mc_lib.data_batch_generator(final_x_train_s, final_y_train_cat_s, cls_2_idx_final_train, shuffle=True,
                                 batch_size=batch_size)

final_train_steps = 2 * final_x_train_s.shape[0] //batch_size

final_cnnlstm_model = mc_lib.create_cnn_lstm_model(max_len=max_len, n_class=N_CLASS)


if not os.path.exists(model_dir):
    os.makedirs(model_dir)

final_model_fp = os.path.join(model_dir, 'final_CNN_BiLSTM_model_w1_h10000.h5')

callbacks_list = [
                 keras.callbacks.ModelCheckpoint(final_model_fp, 
                                                 monitor='val_loss',
                                                 save_best_only=True,)]



start_final_cnnlstm_gen = time.time()
n_final_epoch = 100
hist_final_cnnlstm_gen = final_cnnlstm_model.fit_generator(train_gen, steps_per_epoch=final_train_steps,
                                                           epochs=n_final_epoch, callbacks=callbacks_list, 
                                                           validation_data=(final_x_valid_s, final_y_valid_cat_s))

end_final_cnnlstm_gen = time.time()


##################################################
# Predict classes for test files, and save results 
##################################################
test_img_dir = test2submit_img_o_dir

img_fns_test = mc_lib.list_dir(test_img_dir, suffix=img_sfx)

n_images_test = len(img_fns_test)

x_test2submit = np.zeros(shape=(n_images_test, max_len, 1), dtype=np.float32)

imgfn2id_test= {fn:fn[:-(len(img_sfx) + 1)] for fn in img_fns_test}


# NOTE: maybe use pad sequence to make it to max_len if many files have small size
# For now all images have the same length of max_len


for idx, img_fn in enumerate(img_fns_test):
    img_fn_wp = os.path.join(test_img_dir, img_fn)
    img_bin_stream = np.fromfile(img_fn_wp, dtype='uint8')
    x_test2submit[idx, :, :] = img_bin_stream.reshape(max_len, 1) # Assume they have shape (max_len, 1)


# So the index of sample in x_test2submit is the index of its filename in img_fns_test -> 
# could be used to create the csv file


# list of filename id 0 bin size, need to generate a equal prob prediction for them
equal_prob_pred_ids = [ 
'YvpzOeBSu7Tmia3wKlLf',
'pLY05AFladXWQ9fDZnhb',
'uzRUIAil6dVwWsCvhbKD',
'spRNUv6MFb8ihB9JXk5r',
'TroLhDaQ2qkKe4XmtPEd',
'ZOtweKduNMynmpiG4brh',
'W8aI0V7G5lFTpOgSvjf6',
'N2TJvMjcebxGKq1YDC9k',
'xYr76sCtHa2dD48FiGkK',
'VZ2rzALmJS38uIG5wR1X',
'QpHV1IWD72EnAyB3FowM',
'y5l1PF7qGvsQSDgmRkKn',
'W8VtX0E95TSzxJuGqiI4',
]

loaded_cnnlstm_gen_model = keras.models.load_model(final_model_fp)


start_test_predict = time.time()

y_test2submit_pred = loaded_cnnlstm_gen_model.predict(x_test2submit, batch_size=64)

end_test_predict = time.time()

fnid_test_2_pred = {img_fns_test[i][:-(len(img_sfx)+1)]: y_test2submit_pred[i] for i in range(len(img_fns_test))}

for fn_id in equal_prob_pred_ids:
    fnid_test_2_pred[fn_id] = np.array([1/N_CLASS] * N_CLASS)

y_test2submit_pred_df = \
pd.DataFrame(columns=["Id","Prediction1","Prediction2","Prediction3","Prediction4","Prediction5","Prediction6","Prediction7","Prediction8","Prediction9"])

column_2_cls = {'Prediction'+str(i):i-1 for i in range(1,10)}


test_df_dict = {'Id': list(fnid_test_2_pred.keys())}

for cls_name, cls in column_2_cls.items():
    test_df_dict[cls_name] = np.array([fnid_test_2_pred[id][cls] for id in test_df_dict['Id']])

y_test2submit_pred_df = pd.DataFrame(test_df_dict)


# To know the  average log loss on test data submit file test_files_predict.csv to the kaggle page for the dataset
y_test2submit_pred_df.to_csv('test_files_pred.csv', quoting=csv.QUOTE_NONNUMERIC, index=False)



